{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPDX-FileCopyrightText: 2023 Machine-Learning-OER-Collection\n",
    "# SPDX-License-Identifier: CC-BY-4.0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example code for a gradient boosting classification\n",
    "\n",
    "Welcome back! \n",
    "\n",
    "One important ensemble method is boosting. Boosting methods build basic estimators sequentially and try to reduce the bias of the combined estimator. By combining several weak models, the desired result is a powerful ensemble. Unlike bagging, boosting is a sequential process. The training of the weak learners is sequential (as opposed to parallel), and each model attempts to correct its predecessor. \n",
    "\n",
    "The Model:\n",
    "\n",
    "Boosting is an ensemble method that adapts a sequence of weak learners to repeatedly modified versions of the data. The predictions of the weak learners are then combined by a weighted majority vote to produce the final prediction. The gradient boosting algorithm is a particular boosting method. It builds the model additively. At each step, negative gradient values are fitted to the loss function.\n",
    "\n",
    "<img src=\"../img/gradient_boosting.svg\" alt='Example Gradient Boosting' height=\"500\" width='1000'>\n",
    "\n",
    "- Weak learner: Also called base classifier, like shallow trees, are combined. By default, sklearn uses 100 so-called decision stumps as weak learners.\n",
    "- Loss function or cost function: The goal is to minimize the loss function, which is a global measure of the loss or error occurring in any available decisions.\n",
    "\n",
    "\n",
    "Gradient Boosted Trees often use shallow trees with a depth of one to five, which reduces memory requirements and speeds up prediction. However, they are more sensitive to parameter adjustments than random forests. Nevertheless, they can provide improved accuracy if the parameters are chosen optimally.\n",
    "\n",
    "In addition to the pre-pruning method and the number of trees in the ensemble, the learning rate is another important parameter in gradient boosting. It determines how much each tree tries to correct the mistakes of the previous trees. A higher learning rate allows the trees to make larger corrections and, thus, more complex models. Adding more trees to the ensemble by increasing n_estimators can also lead to increased model complexity, as the model has more opportunities to correct errors in the training data.\n",
    "\n",
    "\n",
    "See [Gradient Tree Boosting](https://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting) in the scikit-learn User Guide for more information about the algorithm.\n",
    "\n",
    "\n",
    "Reference:\n",
    "\n",
    "Example code for a Gradient Boosting Classification model by julia from the repo [machine-learning-OER-Basics](https://github.com/Machine-Learning-OER-Collection/Machine-Learning-OER-Basics) is licensed under [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "We'll start with a basic pipeline which includes the following steps:\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Preparation of the data set:\n",
    "\n",
    "    - Transformation:\n",
    "        - Encode categorical values to numerical (OneHotEncoding etc.)\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Training of the data:\n",
    "    - Split the data into training and testing sets\n",
    "    - Instantiate the Classifier specifying hyperparameters\n",
    "    - Train the Classifier on the training data\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Evaluate the performance of the Classifier\n",
    "    - Classification report\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Used libraries in the code:\n",
    "* [pandas](https://pandas.pydata.org/docs/index.html#) for data analyzing\n",
    "* [scikit-learn](https://scikit-learn.org/stable/index.html) for machine learning\n",
    "* [seaborn](https://seaborn.pydata.org/) for statistical data visualization\n",
    "* [matplotlib](https://matplotlib.org/) for data visualization\n",
    "* [RandomOverSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html) for resampling the data\n",
    "\n",
    "From sklearn:\n",
    "\n",
    "`from sklearn.preprocessing import OneHotEncoder`\n",
    "\n",
    "`from sklearn.model_selection import train_test_split`\n",
    "\n",
    "`from sklearn.tree import GradientBoostingClassifier`\n",
    "\n",
    "`from sklearn.metrics import classification_report`\n",
    "\n",
    "__________________________________________________\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each tree can only provide good predictions on part of the data. So more and more trees are added to iteratively improve performance. <br>\n",
    "We build a strong learner from a set of weak learners. <br>\n",
    "In a binary classification task, a weak learner does at least a little better than random guessing, not a lot better. The probability of error of a strong learner is arbitrarily small. Boosting is focused on reducing bias rather than variance.\n",
    " \n",
    "When training the next classifier in the sequence, data points misclassified by one of the base classifiers are given more weight. Once all classifiers are trained, their predictions are combined using weighted majority voting.\n",
    "\n",
    "In a classification task, the sub-estimator is a regressor, not a classifier. This is because the sub-estimators are being trained for the prediction of (negative) gradients, which are always continuous quantities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in the data set**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the cleaned data set from the EDA for this tutorial.\n",
    "\n",
    "Check for missing values: This downloaded example data set uses question marks (\"?\") for missing values. The parameter na_values converts the question marks to the format NaN (not a number), which pandas can detect. By using the data set from the EDA, the missing values are already handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../../../../kick_after_EDA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsBadBuy</th>\n",
       "      <th>PurchDate</th>\n",
       "      <th>Auction</th>\n",
       "      <th>VehicleAge</th>\n",
       "      <th>Make</th>\n",
       "      <th>Model</th>\n",
       "      <th>Trim</th>\n",
       "      <th>SubModel</th>\n",
       "      <th>Color</th>\n",
       "      <th>Transmission</th>\n",
       "      <th>WheelType</th>\n",
       "      <th>VehOdo</th>\n",
       "      <th>Nationality</th>\n",
       "      <th>Size</th>\n",
       "      <th>TopThreeAmericanName</th>\n",
       "      <th>CurrentAuctionAveragePrice</th>\n",
       "      <th>CurrentAuctionCleanPrice</th>\n",
       "      <th>CurrentRetailAveragePrice</th>\n",
       "      <th>CurrentRetailCleanPrice</th>\n",
       "      <th>BYRNO</th>\n",
       "      <th>VNST</th>\n",
       "      <th>VehBCost</th>\n",
       "      <th>IsOnlineSale</th>\n",
       "      <th>WarrantyCost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1260144000</td>\n",
       "      <td>ADESA</td>\n",
       "      <td>3</td>\n",
       "      <td>MAZDA</td>\n",
       "      <td>MAZDA3</td>\n",
       "      <td>i</td>\n",
       "      <td>4D SEDAN I</td>\n",
       "      <td>RED</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>Alloy</td>\n",
       "      <td>89046</td>\n",
       "      <td>OTHER ASIAN</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>7451.0</td>\n",
       "      <td>8552.0</td>\n",
       "      <td>11597.0</td>\n",
       "      <td>12409.0</td>\n",
       "      <td>21973</td>\n",
       "      <td>FL</td>\n",
       "      <td>7100.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1260144000</td>\n",
       "      <td>ADESA</td>\n",
       "      <td>5</td>\n",
       "      <td>DODGE</td>\n",
       "      <td>1500 RAM PICKUP 2WD</td>\n",
       "      <td>ST</td>\n",
       "      <td>QUAD CAB 4.7L SLT</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>AUTO</td>\n",
       "      <td>Alloy</td>\n",
       "      <td>93593</td>\n",
       "      <td>AMERICAN</td>\n",
       "      <td>LARGE TRUCK</td>\n",
       "      <td>CHRYSLER</td>\n",
       "      <td>7456.0</td>\n",
       "      <td>9222.0</td>\n",
       "      <td>11374.0</td>\n",
       "      <td>12791.0</td>\n",
       "      <td>19638</td>\n",
       "      <td>FL</td>\n",
       "      <td>7600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IsBadBuy   PurchDate Auction  VehicleAge   Make                Model Trim  \\\n",
       "0         0  1260144000   ADESA           3  MAZDA               MAZDA3    i   \n",
       "1         0  1260144000   ADESA           5  DODGE  1500 RAM PICKUP 2WD   ST   \n",
       "\n",
       "            SubModel  Color Transmission WheelType  VehOdo  Nationality  \\\n",
       "0         4D SEDAN I    RED         AUTO     Alloy   89046  OTHER ASIAN   \n",
       "1  QUAD CAB 4.7L SLT  WHITE         AUTO     Alloy   93593     AMERICAN   \n",
       "\n",
       "          Size TopThreeAmericanName  CurrentAuctionAveragePrice  \\\n",
       "0       MEDIUM                OTHER                      7451.0   \n",
       "1  LARGE TRUCK             CHRYSLER                      7456.0   \n",
       "\n",
       "   CurrentAuctionCleanPrice  CurrentRetailAveragePrice  \\\n",
       "0                    8552.0                    11597.0   \n",
       "1                    9222.0                    11374.0   \n",
       "\n",
       "   CurrentRetailCleanPrice  BYRNO VNST  VehBCost  IsOnlineSale  WarrantyCost  \n",
       "0                  12409.0  21973   FL    7100.0             0          1113  \n",
       "1                  12791.0  19638   FL    7600.0             0          1053  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the data frame\n",
    "pd.set_option('display.max_columns', None) # if not set, displayed columns will be truncated\n",
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First step - Transformation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the categorical values | One-Hot-Encoding\n",
    "\n",
    "As scikit-learn states in the documentation, the tree-based algorithms currently do not support categorical variables. To use both - numerical and categorical features - for the algorithm, the categorical values must be transformed into numeric values. Depending on the value type, the [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn-preprocessing-onehotencoder), [OrdinalEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn-preprocessing-ordinalencoder), or [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) (NOTE: Use LabelEncoder only for the target variable! The target variable for this data set is numerical, so there is no need for encoding.) comes in helpful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the OneHotEncoder from sklearn.preprocessing. Next, we determine the categorial features and assign them to a new data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Define the categorical feature\n",
    "categorical_features = ['Auction', 'Make', 'Model', 'Trim', 'SubModel', 'Color', 'Transmission',\n",
    "       'WheelType', 'Nationality', 'Size', 'TopThreeAmericanName', 'VNST']\n",
    "\n",
    "# Write categorical columns to new data frame\n",
    "categorical_data = df[categorical_features]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we assign the OneHotEncoder to the variable encoder. The `fit()`method of the encoder is used to train the categorical features. Finally, we use the `transform()` method to transform the categorical features into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign OneHotEncoder to variable\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Encoder learns the categories\n",
    "encoder.fit(categorical_data)\n",
    "\n",
    "# Transform categorical data into encoded array\n",
    "encoded_data = encoder.transform(categorical_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame with encoded data\n",
    "\n",
    "The method `toarray()` converts the encoded data into a numpy array. The array is then converted into a data frame with `pd.DataFrame()`. The column names are extracted from the encoder with `get_feature_names_out()`.\n",
    "\n",
    "Using the `concat()` method, the encoded data is concatenated with the original data set. The original columns are dropped with the `drop()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of features after using OneHotEncoder: 2144\n"
     ]
    }
   ],
   "source": [
    "# Create df\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(), columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# Concatenate the encoded data frame with the remaining columns of original data frame\n",
    "df_encoded = pd.concat([df.drop(categorical_features, axis=1), encoded_df], axis=1)\n",
    "print(f'Amount of features after using OneHotEncoder: {df_encoded.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OneHotEncode method converts the categorical values to numerical values. The method creates a new column for each category and assigns a 1 or 0 to the column. The 1 represents the existence of the category, the 0 represents the non-existence. We have 12 categorical features in our data set, all with many different values. Dimensionality reduction is not covered in this notebook for now. You can find more information on [methods here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) or create a notebook and contribute to this repository."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the target variable (y) from the features (X)**\n",
    "\n",
    "The [Gradient Boosting Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn-ensemble-gradientboostingclassifier) takes X and y as input. The target variable (y) is the variable that should be predicted. The features (X) are the variables used to predict the target variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the encoded df into X and y\n",
    "X = df_encoded.drop('IsBadBuy', axis=1) # Drop the target column from the features\n",
    "y = df_encoded['IsBadBuy']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second step - Training of the data\n",
    "\n",
    "Split the data into training and testing sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function splits the data set into training and test sets. The test_size parameter specifies the proportion of the test set. You can also split the data set into 25% test and 75% training data by setting the parameter to 0.25. If you use the default parameters, you're all set for the first run, so don't worry.\n",
    "\n",
    "The random_state parameter ensures that the split is always the same. You can set it to any integer. If you don't set the parameter, the split will differ each time you run the code. We'll decide to set the parameter to 42.\n",
    "\n",
    "X_train and y_train are the _training_ sets for the features and the target variable. X_test and y_test are the _test_ sets for the features and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape Counter({0: 42568, 1: 5982})\n",
      "Testing set shape Counter({0: 20988, 1: 2926})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(f'Training set shape {Counter(y_train)}')\n",
    "print(f'Testing set shape {Counter(y_test)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the GradientBoostingClassifier**\n",
    "\n",
    "Let's go line by line through the code:\n",
    "\n",
    "We'll import the GradientBoostingClassifier from sklearn.ensemble. \n",
    "\n",
    "Instantiate the Classifier specifying parameters: We'll use almost all the default parameters now. You can find more information about the parameters in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn-ensemble-gradientboostingclassifier).\n",
    "\n",
    "Let's dive into the parameters we set:\n",
    "\n",
    "* The `loss` parameter is set to the default value log_loss. This is called [logistic loss or cross-entropy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html). It quantifies the difference between the predicted probabilities (y_pred) and the true labels (y_true) for a given sample. \n",
    "\n",
    "* The `learning_rate` parameter is set to the default of 0.1. The learning rate reduces each tree's contribution by the learning_rate. Between learning_rate and n_estimators is a trade-off. A higher learning rate means that each individual tree can make stronger corrections. That allows for more complex models.\n",
    "\n",
    "* `n_estimators` is set to the default of 100. This parameter determines the number of boosting stages to perform. Gradient boosting is quite robust to overfitting. Therefore, a higher number typically results in better performance. Values have to be in the range between 1 and an infinity integer.\n",
    "\n",
    "* For the parameter `subsample`, we use the default of 1.0. This parameter determines the fraction of samples to be used for fitting the individual base learners. If it is less than 1.0, this results in Stochastic Gradient Boosting. This parameter interacts with the n_estimators parameter.  Choosing subsample < 1.0 will reduce the variance and increase the bias. Values must range from 0.0 to 1.0.\n",
    "\n",
    "* We use the default for `criterion`, which is ’friedman_mse’. This function measures the quality of a split. The criterion used is 'friedman_mse' for the mean squared error with an improvement score by [Friedman](https://projecteuclid.org/journals/annals-of-statistics/volume-29/issue-5/Greedy-function-approximation-A-gradient-boosting-machine/10.1214/aos/1013203451.full). The default is generally the best, as it may give a better approximation.\n",
    "\n",
    "* For the `min_samples_split`, we use the default of 2. This determines the minimum number of samples necessary to split an internal node.\n",
    "\n",
    "* The `min_samples_leaf` parameter, here 1, determines the minimum number of training samples that must be present in each leaf node of the decision tree. If a split point is considered at any depth, it is only allowed if the left and right branches resulting from that split have at least 'min_samples_leaf' training samples each.\n",
    "\n",
    "* We use the default of 0.0 for the `min_weight_fraction_leaf` parameter of the sum of weights (of all input samples) required to be at a leaf node. The samples have the same weight if sample_weight is not specified.\n",
    "\n",
    "* The `max_depth` is the depth of each regression estimator, here 3. The maximum depth limits the number of nodes in the tree. This parameter can be adjusted for best performance. The best value depends on the interaction of the input variables. If it is set to None, the nodes will be expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "\n",
    "* The `min_impurity_decrease` parameter is set to the default value of 0.0. A node will be split if the split causes a decrease in impurity greater than or equal to this value.\n",
    "\n",
    "* The `init` parameter, here None, specifies an estimator used to generate initial predictions during the boosting process. It must implement both the _fit_ and _predict_proba_ methods. By default, the boosting algorithm uses a DummyEstimator to predict class priors, and setting it to 'zero' will initialize raw predictions to zero.\n",
    "\n",
    "* The `random_state` parameter controls the random seed used for the tree estimator at each boosting iteration and for random feature permutation at each split. Additionally, it influences the random splitting of the training data to create a validation set when 'n_iter_no_change' is specified. We set the parameter to integer 42.\n",
    "\n",
    "* The `max_features` parameter determines the number of features considered for finding the best split. We use the default value None, using all features (n_features). Using a smaller max_features than n_features value reduces variance and increases bias. The algorithm continues searching for a split until at least one valid partition of the node samples is found, even if it needs to inspect more than max_features features.\n",
    "\n",
    "* We set `verbose` to 0, so no messages are printed. Setting it to >= 1 will print messages for progress and performance.\n",
    "\n",
    "* `max_leaf_nodes` is set to the default value of None. This parameter specifies the maximum number of leaf nodes. The default value of None means that the number of leaf nodes is unlimited.\n",
    "\n",
    "* The parameter `warm_start` is set to False. The parameter controls whether the solution from the previous fit call should be reused, adding more estimators to the existing ensemble.\n",
    "\n",
    "* The `validation_fraction` parameter, with a default value of 0.1, represents the proportion of training data reserved as the validation set for early stopping. It is only utilized when the 'n_iter_no_change' parameter is set to an integer and should be within the range (0.0, 1.0).\n",
    "\n",
    "* The `n_iter_no_change` parameter determines whether early stopping is used during training if the validation score does not improve. If set to a number, a portion of the training data is reserved for validation, and training is stopped if the validation score does not improve for the last 'n_iter_no_change' iterations. The data partitioning for validation is done in layers. By default, it is set to None, which disables early stopping.\n",
    "\n",
    "* The `tol` parameter represents the tolerance for early stopping. If 'n_iter_no_change' is set to a number, training will stop when the loss does not improve by at least 'tol' for 'n_iter_no_change' iterations consecutively. The default value _1e-4_ means that training stops if the loss does not improve at least 0.0001 in the specified number of iterations.\n",
    "\n",
    "* For `ccp_alpha`, we leave the default value of 0.0. The ccp_alpha parameter, also known as the complexity parameter, regulates the pruning of subtrees in decision trees.\n",
    "\n",
    "\n",
    "As mentioned, if you use the default parameters, you don't need to specify them in the GradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "# Instantiate GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier(random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we train the model. The `fit()` method trains the model on the training data X_train and y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(random_state=42)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(random_state=42)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model to training set\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `predict()` predicts the target variable for the test data X_test. The predicted values are assigned to the variable `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set labels\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third step - Evaluation of the performance\n",
    "\n",
    "Let's compute the `classification_report()` using the true labels (y_test) and the predicted labels (y_pred). \n",
    "\n",
    "The precision, recall and f1_score are explained [here](/supervised_learning/classification/k_nearest_neighbors/text/f1_score.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0    0.87834   0.99933   0.93494     20988\n",
      "     Class 1    0.60000   0.00718   0.01418      2926\n",
      "\n",
      "    accuracy                        0.87794     23914\n",
      "   macro avg    0.73917   0.50325   0.47456     23914\n",
      "weighted avg    0.84429   0.87794   0.82228     23914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predictions on the test data by using the trained model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "target_names=['Class 0', 'Class 1']\n",
    "print(classification_report(y_test, y_pred, target_names=target_names, digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the basic pipeline for a Gradient Boosting Classifier. For Class 0, the F1-score is 0.93, which is high and indicates a good balance between precision and recall. However, for Class 1, the F1-score is only 0.01, showing poor performance due to a recall of 0.01. Let's see if we can improve the performance by balancing the data set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imbalanced data set\n",
    "\n",
    "The manipulation of [imbalanced data](/poc/supervised_learning/decision_tree/explainer/data_sets.md) sets has an impact on the performance of the model. This tutorial uses the [RandomOverSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html) method to balance the data set. The method randomly selects samples with replacement to match the number of samples of the  minority class. For different methods, see the [imbalanced-learn](https://imbalanced-learn.org/stable/references/index.html) documentation.\n",
    "\n",
    "**RandomOverSampler**\n",
    "\n",
    "The RandomOverSampler technique balances the data by randomly picking samples from the minority class with replacement and adds them to the training data set. The test data set remains unchanged.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the oversampling method to our data set. NOTE: The oversampling method should only be applied to the training data set. So we need to split the data set into training and test data set first. We use the same parameter we used in our basic model.\n",
    "\n",
    "We name the split data set `X_train_ros, X_test_ros, y_train_ros, y_test_ros` for better distinction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_ros, X_test_ros, y_train_ros, y_test_ros = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following parameter:\n",
    "* `sampling_strategy='auto'` specifies that all classes but the minority class are resampled; as we have only two classes here, it is equivalent to 'not minority'. The number of samples in the minority class will be equal to the number of samples in the majority class.\n",
    "* The parameter `random_state=42` ensures that the results are reproducible.\n",
    "\n",
    "We import the `RandomOverSampler()` method from the `imblearn.over_sampling` library and assign the method to the variable ros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "# Instantiate RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42, sampling_strategy='auto')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we call the method `fit_resample()` on the training data set. The method returns the resampled data set and the resampled labels. We assign the resampled data set to the variable X_train_resampled and the resampled labels to the variable y_train_resampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data set shape Counter({0: 42568, 1: 5982})\n",
      "Resampled data set shape Counter({0: 42568, 1: 42568})\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to the training set\n",
    "X_train_resampled, y_train_resampled = ros.fit_resample(X_train_ros, y_train_ros)\n",
    "\n",
    "from collections import Counter\n",
    "print('Original data set shape %s' % Counter(y_train))\n",
    "print(f'Resampled data set shape {Counter(y_train_resampled)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes 0 and 1 are now equally represented in the training set.\n",
    "\n",
    "Let's train the model again with the oversampled data set and the same parameter settings. We name it ros_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Instantiate GradientBoostingClassifier\n",
    "ros_model = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Fit model to training set\n",
    "ros_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Predict test set labels\n",
    "y_pred_ros = ros_model.predict(X_test_ros)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fourth step - Evaluation of the performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.93      0.63      0.75     20988\n",
      "     Class 1       0.20      0.68      0.31      2926\n",
      "\n",
      "    accuracy                           0.63     23914\n",
      "   macro avg       0.57      0.65      0.53     23914\n",
      "weighted avg       0.84      0.63      0.70     23914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Predictions on the test data by using the trained model\n",
    "y_pred_ros = ros_model.predict(X_test_ros) # create variable predicted labels (y_pred_ros)\n",
    "\n",
    "target_names=['Class 0', 'Class 1']\n",
    "print(classification_report(y_test_ros, y_pred_ros, target_names=target_names)) # Taking into account the true labels (y_test_ros) and the predicted labels (y_pred_ros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the basic pipeline, the recall for the minority class has increased from 0.00 to 0.68. That is a good score considering that all we did was balance the data set.\n",
    "\n",
    "The precision for class 0 is quite good. When it predicts class 0, it is correct 93% of the time. However, the recall (0.63) is relatively low, indicating that it doesn't capture all class 0 instances, missing almost 40% of them.\n",
    "\n",
    "The precision for class 1 is very low at 0.20, indicating that the model is correct only 20% of the time when predicting class 1. The low precision for Class 1 could be the reason for the low overall F1 score.\n",
    "\n",
    "Note, that applying a boosting method requires a large data set so each learner as enough samples to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes this notebook. The methods you can use for improvement are various and cover dimensionality reduction, feature selection, feature engineering, and many more. Again, always remember that a slight variation in the data can lead to a completely different model.\n",
    "\n",
    "Let's return to our business case. The objective of the task is to predict whether the vehicle purchased at auction is a bad buy.\n",
    "If we compare the three models, we can see this is a challenging task. Nevertheless, we get results that provide a basis for further methods. Extracting the first results from the available data is possible with only a few lines of code. But once these lines of code are written, the iterative process of improving the model begins. There should always be close collaboration with the business side/stakeholders, as there are changes in the data or the set target. Accordingly, information must be obtained and updated on an ongoing basis. \n",
    "\n",
    "The next step would be to look at the generalizability for each model with [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-evaluating-estimator-performance) and trying to improve the performance of the model by examine the [most important features](https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance) and [parameter](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn-model-selection-gridsearchcv). Gradient Boosting shows good results when the parameters are tuned carefully."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
